#This is only CNN, with SMOTE oversampling

#python imports
import numpy as np
import codecs
import nltk
from nltk.corpus import stopwords
from os import listdir
import random
import scipy
import h5py

#keras imports
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, Flatten
from keras.layers import LSTM, Flatten
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.models import model_from_json


#sklearn imports
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import f1_score
from sklearn.utils import class_weight
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, precision_score, recall_score

#xgboost imports
from xgboost import XGBClassifier

#SMOTE import for oversampling
from imblearn.over_sampling import SMOTE



def train_and_predict(X, Y):
	np.random.seed(7)
	top_words = 5000
	max_review_length = 150
	
	#Convert text into tfidf features
	count_vect = CountVectorizer()
	X_counts = count_vect.fit_transform(X)
	tf_transformer = TfidfTransformer()
	X_tf = tf_transformer.fit_transform(X_counts)

	#Perform dimensionality reduction using svd
	svd = TruncatedSVD(n_components = 100, n_iter=7, random_state=50)
	X = scipy.sparse.csc_matrix(X_tf)
	X = svd.fit_transform(X)

	#Perform oversampling using SMOTE
	sm = SMOTE(ratio = 'minority', random_state=1, kind='regular', k_neighbors = 5)
	X, Y = sm.fit_sample(X, Y)
	
	#Split the dataset for trainng and testing using a 67--33 training - test split
	X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 1, shuffle = True)
	Y_train = np.array(Y_train)

	#Train XGBoost classifier
	model = XGBClassifier(learning_rate=0.2, n_estimators=1000, max_depth=5, ) 
	model.fit(X_train, Y_train)
	
	#Predict on test data
	Y_pred = model.predict(X_test)
	predictions = [round(value) for value in Y_pred]

	print("F Score: %.2f%%" % (f1_score(Y_test, predictions) * 100.0))
	print('Precision Score: %.2f%%' % (precision_score(Y_test, predictions) * 100.0))
	print('Recall Score: %.2f%%' % (recall_score(Y_test, predictions) * 100.0))

#Take input folder and return dataset with labels
def generateData(dataFolder):
	stops = set(stopwords.words("english"))  
	X = []
	Y = []
	sentence = ''
	relevance = 0
	for fileName in listdir(dataFolder):
		f = codecs.open(dataFolder + fileName, encoding='utf-8')
		#f = open(dataFolder + fileName, 'r')
		#with open(dataFolder + fileName, 'r') as f:
		for line in f:
			if '\n' in line and len(line) == 2: #End of a Sentence
				if sentence != '':
					X.append(sentence)
					Y.append(relevance)
				#Reset sentence and relevance for next line
				sentence = ''
				relevance = 0 
			else:	#Same sentence still being continued
				token = line.split(' ')[0].lower()
				#Keep only words of letters and not stop words
				if token not in stops and token not in ",!.:<>":
					if sentence == '': 
						sentence = token
					else: 
						sentence += ' ' + token
				if line[:-1].split(' ')[-1] != 'O': #if it is annotated, then it is relevant 
					relevance = 1

	return X,Y
#Get the training data
X, Y = generateData("training_material/data/tokenized/")

train_and_predict(X, Y)
