#This is Random Forest Classifier with oversampling
import codecs
import nltk
from nltk.corpus import stopwords
from os import listdir
import numpy as np
import random
import re
import pickle


from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_recall_fscore_support
from sklearn.model_selection import train_test_split

from imblearn.over_sampling import SMOTE

def generateData(dataFolder):
	stops = set(stopwords.words("english"))  
	X = []
  	Y = []
  	sentence = ''
  	relevance = 0
  	for fileName in listdir(dataFolder):
		f = open(dataFolder + fileName, 'r')
		for line in f:
			if '\n' in line and len(line) == 2: #End of a Sentence
				if sentence != '':
					X.append(sentence)
					Y.append(relevance)
				sentence = ''
				relevance = 0 
			else:	#Same sentence still being continued
				token = line.split(' ')[0].lower()
				#Keep only words of letters and not stop words and token is not only made of numbers
				#if token not in stops and token not in ",!.:<>":
				reNumbers = re.findall(r'[0-9\.]+', token)
				if token not in ",!.:<>" and token not in stops:
					token = re.sub(r'[^A-Za-z]', '', token)
					if len(reNumbers) > 0:
						if reNumbers[0] != token:
							if sentence == '': 
								sentence = token
							else: 
								sentence += ' ' + token
					else:
						if sentence == '':
							sentence = token
						else:
							sentence += ' ' + token
				if line[:-1].split(' ')[-1] != 'O': #if it is annotated, then it is relevant 
					relevance = 1
  	return X,Y

#Get the training data
X, Y = generateData("training_material/data/tokenized/")


#Convert text into features using bag of words model
vectorizer = CountVectorizer(analyzer = "word",   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = None,   \
                             max_features = 500) 
X_features = vectorizer.fit_transform(X)
X_features = X_features.toarray()

#Perform oversampling using SMOTE
sm = SMOTE(ratio = 'minority', random_state=1, kind='regular', k_neighbors = 5)
X_res, Y_res = sm.fit_sample(X_features, Y)

print len(X_res), " is the dataset size after oversampling with SMOTE"

#Split the dataset for trainng and testing using a 67--33 training - test split
X_train_features, X_test_features, Y_train, Y_test = train_test_split(X_res, Y_res, test_size = 0.33, random_state = 1, shuffle = True)

#Train the random forest classifier on the features
forest = RandomForestClassifier(n_estimators = 100, class_weight = "balanced")
forest = forest.fit(X_train_features, Y_train)

#Predict the result on test set
result = forest.predict(X_test_features)

i = 0
count = 0
tp = 0.0
fp = 0.0
tn = 0.0
fn = 0.0
for each in result:
	#print each, type(each)
	t = Y_test[i]
	if each != t:
		#print each, Y_test[i]
		count = count + 1

		if each == 0 and t == 1:	
			fn = fn + 1
		if each == 1 and t == 0:
			fp = fp + 1
	elif each == t:
		if each == 0 and t == 0:
			tn = tn + 1
		if each == 1 and t == 1:
			tp = tp + 1
	i = i + 1

#Print accuracy, precision , recall, fscore and support
print "Incorrectly predicted ", count, "out of ", i, tp, fp, fn, tn
print "Accuracy is ", (float)((i - count) / (float)(i))
print "Precision = ", (float)(tp / (tp + fp)), " Recall is ", (float)(tp / (tp + fn))

y_true = np.array(Y_test)
y_pred = np.array(result)
print precision_recall_fscore_support(y_true, y_pred, pos_label = 1, average = 'binary')
