#This is LSTM with SMOTE, gives f-score of 0.82 on unseen data (4757 samples)

#Python imports
import numpy as np
import codecs
from nltk.corpus import stopwords
from os import listdir
import random

#Keras imports
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM, Flatten
from keras.layers.embeddings import Embedding
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing import sequence

#Scikit learn imports
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import f1_score
from sklearn.utils import class_weight
from sklearn.model_selection import train_test_split

#SMOTE for oversampling
from imblearn.over_sampling import SMOTE


def try_lstm(X, Y):
	np.random.seed(7)
	top_words = 5000
	max_length = 100
	
	
	#Use one-hot encoding to encode the sentences
	encoded_sentences = [one_hot(d, top_words) for d in X]

	#Pad sentences if the length is greater than maxlen and truncate if length is lesser tan maxlen
	padded_sentences = pad_sequences(encoded_sentences, maxlen = max_length, padding='post')
	
	#Perform oversampling using SMOTE
	sm = SMOTE(ratio = 'minority', random_state=1, kind='regular', k_neighbors = 5)
	X_res, Y_res = sm.fit_sample(padded_sentences, Y)
	
	#Split the dataset for trainng and testing using a 67--33 training - test split
	X_train, X_test, y_train, y_test = train_test_split(X_res, Y_res, test_size = 0.33, random_state = 1, shuffle = True)
	
	#calculate class weights to for using them in loss function
	myclass_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)
	class_weight_dict = dict(enumerate(myclass_weight))
	
	# create the model
	#Use word embeddings of 32 dimensions, fewer dimensions in this case provide better performance due to smaller dataset
	embedding_vecor_length = 32
	model = Sequential()

	#Add the embedding layer at the beginning to accept input of max length
	model.add(Embedding(top_words, embedding_vecor_length, input_length=max_length))

	#Add three LSTM layers with 256, 128 and 16 units respectively and a dropout of 0.2.
	model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2, name='lstm1', return_sequences=True))
	model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, name='lstm2', return_sequences=True))
	model.add(LSTM(16, dropout=0.2, recurrent_dropout=0.2, name='lstm3', return_sequences=True))
	model.add(Flatten())

	#Add 1 unit NN for classification
	model.add(Dense(1, activation='sigmoid'))

	#Train the model on the dataset
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	print(model.summary())
	model.fit(X_train, y_train, epochs=1, batch_size=64, class_weight=class_weight_dict)
	
	# Final evaluation of the model
	scores = model.evaluate(X_test, y_test, verbose=0)

	#Predict the classes and the print Precision, Recall, Fscore and Accuracy
	a = model.predict_classes(X_test, verbose=1)
	print(precision_recall_fscore_support(y_test, a))
	print("F-Score: ", f1_score(y_test, a))	
	print("Accuracy: %.2f%%" % (scores[1]*100))

def generateData(dataFolder):
	stops = set(stopwords.words("english"))  
	X = []
	Y = []
	sentence = ''
	relevance = 0
	for fileName in listdir(dataFolder):
		f = codecs.open(dataFolder + fileName, encoding='utf-8')
		for line in f:
			if '\n' in line and len(line) == 2: #End of a Sentence
				if sentence != '':
					X.append(sentence)
					Y.append(relevance)
				#Reset sentence and relevance for next line
				sentence = ''
				relevance = 0 
			else:	#Same sentence still being continued
				token = line.split(' ')[0].lower()
				#Keep only words of letters and not stop words
				if token not in stops and token not in ",!.:<>":
					if sentence == '': 
						sentence = token
					else: 
						sentence += ' ' + token
				if line[:-1].split(' ')[-1] != 'O': #if it is annotated, then it is relevant 
					relevance = 1

	return X,Y

#Get the training data
X, Y = generateData("training_material/data/tokenized/")
try_lstm(X, Y)
