#This is only CNN with undersampling

#Python imports
import numpy as np
import codecs
from nltk.corpus import stopwords
from os import listdir
import random

#Keras imports
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, Flatten
from keras.layers import LSTM, Flatten
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences


#Scikit learn imports
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import f1_score
from sklearn.utils import class_weight



def try_cnn(X, Y):
	np.random.seed(7)
	top_words = 5000
	max_length = 150
	
	#Use one-hot encoding to encode the sentences
	encoded_sentences = [one_hot(d, top_words) for d in X]

	#Pad sentences if the length is greater than maxlen and truncate if length is lesser tan maxlen
	padded_sentences = pad_sequences(encoded_sentences, maxlen = max_length, padding='post')
    
	#Split the data into training and test sets
    	X_train = padded_sentences[:3000]
    	X_test = padded_sentences[3000:]
    	y_train = Y[:3000]
    	y_test = Y[3000:]

    	#calculate class weights to for using them in loss function
	myclass_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)
	class_weight_dict = dict(enumerate(myclass_weight))

	# Creation of the model
	#Use word embeddings of 32 dimensions, fewer dimensions in this case provide better performance due to smaller dataset
	embedding_vecor_length = 100
	model = Sequential()

	#Add the embedding layer at the beginning to accept input of max length
	model.add(Embedding(top_words, embedding_vecor_length, input_length=max_length))


	#Create a CNN architecture with 2 1D convolutional layers with 128 filters and filter size 5. Relu activation is used
	model.add(Conv1D(128, 5, activation='relu', padding='same'))
	model.add(Conv1D(128, 5, activation='relu', padding='same'))

	#Add a maxpooling layer with size 5
	model.add(MaxPooling1D(5))

	#Add dropout with rate 0.5 to prevent overfitting
	model.add(Dropout(0.5))
	model.add(Flatten())

	#Add two Dense NN layers with 128 units and 1 units for classification with a dropout layer to prevent overfitting
	model.add(Dense(128, activation='relu', name='dense1'))
	model.add(Dropout(0.5))
	model.add(Dense(1, activation='sigmoid'))

	#Train the model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	print(model.summary())
	model.fit(X_train, y_train, epochs=5, batch_size=64, class_weight=class_weight_dict)

	# Evaluation of the model
	scores = model.evaluate(X_test, y_test, verbose=0)

	#Predict the classes and the print Precision, Recall, Fscore and Accuracy
	a = model.predict_classes(X_test, verbose=1)
	print(precision_recall_fscore_support(y_test, a))
	print("F-Score: ", f1_score(y_test, a))	
	print("Accuracy: %.2f%%" % (scores[1]*100))

#This function reads the training data files and returns two arrays of the sentences and their class
def generateData(dataFolder):
	stops = set(stopwords.words("english"))  
	X = []
	Y = []
	sentence = ''
	relevance = 0
	for fileName in listdir(dataFolder):
		f = codecs.open(dataFolder + fileName, encoding='utf-8')
		for line in f:
			if '\n' in line and len(line) == 2: #End of a Sentence
				if sentence != '':
					X.append(sentence)
					Y.append(relevance)
				#Reset sentence and relevance for next line
				sentence = ''
				relevance = 0 
			else:	#Same sentence still being continued
				token = line.split(' ')[0].lower()
				#Keep only words of letters and not stop words
				if token not in stops and token not in ",!.:<>":
					if sentence == '': 
						sentence = token
					else: 
						sentence += ' ' + token
				if line[:-1].split(' ')[-1] != 'O': #if it is annotated, then it is relevant 
					relevance = 1

	return X,Y

#Get the training data
X, Y = generateData("training_material/data/tokenized/") #this is the relative path of the SemEval training data

#Find number of positives and negatives in the dataset and make it balanced
pos = []
neg = []
i = 0
for i in range(0, len(X)):
        if Y[i] == 1:
                pos.append((X[i], Y[i]))
        else:
                neg.append((X[i], Y[i]))

data = random.sample(pos, 2000)
data = data + random.sample(neg, 2000)
random.shuffle(data)

train = data[:3000]
test = data[3000:]
X_train = []
Y_train = []
X_test = []
Y_test = []
for each in train:
        X_train.append(each[0])
        Y_train.append(each[1])


for each in test:
        X_test.append(each[0])
        Y_test.append(each[1])

try_cnn(X_train + X_test, Y_train + Y_test)
