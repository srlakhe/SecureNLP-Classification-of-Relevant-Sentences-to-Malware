#This is only CNN, with SMOTE oversampling

#Python imports
import numpy as np
import random
import codecs
from nltk.corpus import stopwords
from os import listdir

#Keras imports
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, Flatten
from keras.layers import LSTM, Flatten
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences

#Scikit learn imports
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import f1_score
from sklearn.utils import class_weight
from sklearn.model_selection import train_test_split

#SMOTE for oversampling
from imblearn.over_sampling import SMOTE




def try_cnn(X, Y):
	np.random.seed(7)
	top_words = 5000
	max_review_length = 150
	
	#Use one-hot encoding to encode the sentences
	encoded_sentences = [one_hot(d, top_words) for d in X]

	#Pad sentences if the length is greater than maxlen and truncate if length is lesser tan maxlen
	padded_sentences = pad_sequences(encoded_sentences, maxlen = max_review_length, padding='post')
	
	#Perform oversampling using SMOTE
	sm = SMOTE(ratio = 'minority', random_state=1, kind='regular', k_neighbors = 5)
	X_res, Y_res = sm.fit_sample(padded_sentences, Y)
	
	#Split the dataset for trainng and testing using a 67--33 training - test split
	X_train, X_test, y_train, y_test = train_test_split(X_res, Y_res, test_size = 0.33, random_state = 1, shuffle = True)

	#calculate class weights to for using them in loss function
	myclass_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)
	class_weight_dict = dict(enumerate(myclass_weight))
	
	# create the model
	#Use word embeddings of 32 dimensions, fewer dimensions in this case provide better performance due to smaller dataset
	embedding_vecor_length = 100
	model = Sequential()

	#Add the embedding layer at the beginning to accept input of max length
	model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))

	#Create a CNN architecture with 2 1D convolutional layers with 128 filters and filter size 5. Relu activation is used
	model.add(Conv1D(128, 5, activation='relu', padding='same'))
	model.add(Conv1D(128, 5, activation='relu', padding='same'))

	#Add a maxpooling layer with size 5
	model.add(MaxPooling1D(5))

	#Add dropout with rate 0.5 to prevent overfitting
	model.add(Dropout(0.5))
	model.add(Flatten())

	#Add two Dense NN layers with 128 units and 1 units for classification with a dropout layer to prevent overfitting
	model.add(Dense(128, activation='relu', name='dense1'))
	model.add(Dropout(0.5))
	model.add(Dense(1, activation='sigmoid'))

	#Train the model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	print(model.summary())
	model.fit(X_train, y_train, epochs=3, batch_size=64, class_weight=class_weight_dict)
	
	# Final evaluation of the model
	scores = model.evaluate(X_test, y_test, verbose=0)

	#Predict the classes and the print Precision, Recall, Fscore and Accuracy
	a = model.predict_classes(X_test, verbose=1)
	print(precision_recall_fscore_support(y_test, a))
	print("F-Score: ", f1_score(y_test, a))	
	print("Accuracy: %.2f%%" % (scores[1]*100))

def generateData(dataFolder):
	stops = set(stopwords.words("english"))  
	X = []
	Y = []
	sentence = ''
	relevance = 0
	for fileName in listdir(dataFolder):
		f = codecs.open(dataFolder + fileName, encoding='utf-8')
		#f = open(dataFolder + fileName, 'r')
		#with open(dataFolder + fileName, 'r') as f:
		for line in f:
			if '\n' in line and len(line) == 2: #End of a Sentence
				if sentence != '':
					X.append(sentence)
					Y.append(relevance)
				#Reset sentence and relevance for next line
				sentence = ''
				relevance = 0 
			else:	#Same sentence still being continued
				token = line.split(' ')[0].lower()
				#Keep only words of letters and not stop words
				if token not in stops and token not in ",!.:<>":
					if sentence == '': 
						sentence = token
					else: 
						sentence += ' ' + token
				if line[:-1].split(' ')[-1] != 'O': #if it is annotated, then it is relevant 
					relevance = 1

	return X,Y
#Get the training data
X, Y = generateData("training_material/data/tokenized/")
try_cnn(X, Y)
