#This is Random Forest with undersampling

#Python Imports
import codecs
import nltk
from nltk.corpus import stopwords
from os import listdir
import numpy as np
import random

#Scikit learn imports
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import f1_score
from sklearn.utils import class_weight


def generateData(dataFolder):
	stops = set(stopwords.words("english"))  
	X = []
	Y = []
	sentence = ''
	relevance = 0
	for fileName in listdir(dataFolder):
		f = codecs.open(dataFolder + fileName, encoding='utf-8')
		for line in f:
			if '\n' in line and len(line) == 2: #End of a Sentence
				if sentence != '':
					X.append(sentence)
					Y.append(relevance)
				#Reset sentence and relevance for next line
				sentence = ''
				relevance = 0 
			else:	#Same sentence still being continued
				token = line.split(' ')[0].lower()
				#Keep only words of letters and not stop words

				if token not in stops and token not in ",!.:<>":
					if sentence == '': 
						sentence = token
					else: 
						sentence += ' ' + token
				if line[:-1].split(' ')[-1] != 'O': #if it is annotated, then it is relevant 
					relevance = 1

	return X,Y

#Get the training data
X, Y = generateData("training_material/data/tokenized/")

#Find number of positives and negatives in the dataset and make it balanced
pos = []
neg = []
i = 0
for i in range(0, len(X)):
        if Y[i] == 1:
                pos.append((X[i], Y[i]))
        else:
                neg.append((X[i], Y[i]))

data = random.sample(pos, 2000)
data = data + random.sample(neg, 2000)
random.shuffle(data)

train = data[:3000]
test = data[3000:]
X_train = []
Y_train = []
X_test = []
Y_test = []
for each in train:
        X_train.append(each[0])
        Y_train.append(each[1])


for each in test:
        X_test.append(each[0])
        Y_test.append(each[1])

#Conver text into features using bag of words model
vectorizer = CountVectorizer(analyzer = "word",   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = None,   \
                             max_features = 4000) 
X_train_features = vectorizer.fit_transform(X_train)
X_train_features = X_train_features.toarray()

X_test_features = vectorizer.fit_transform(X_test)
X_test_features = X_test_features.toarray()

## Random Forest Classifier Implementation Below

#Train the random forest classifier on the features
forest = RandomForestClassifier(n_estimators = 200, class_weight = "balanced")
forest = forest.fit(X_train_features, Y_train)

#Predict the result on test set
result = forest.predict(X_test_features)
i = 0
count = 0
tp = 0
fp = 0
tn = 0
fn = 0
for each in result:
	t = Y_test[i]
	if each != t:
		count = count + 1

		if each == 0 and t == 1:	
			fn = fn + 1
		if each == 1 and t == 0:
			fp = fp + 1
	elif each == t:
		if each == 0 and t == 0:
			tn = tn + 1
		if each == 1 and t == 1:
			tp = tp + 1
	i = i + 1

#Print accuracy, precision , recall, fscore and support
print( "Incorrectly predicted ", count, "out of ", i, tp, fp, fn, tn)

y_true = np.array(Y_test)
y_pred = np.array(result)
print (precision_recall_fscore_support(y_true, y_pred, pos_label = 1, average = 'binary'))
